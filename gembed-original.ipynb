{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 202929  Vocab Size 44346\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/news.txt\"\n",
    "\n",
    "vocabulary = tf.compat.as_str(open(filename, encoding='utf8').read()[:1000_000]).split()\n",
    "vocab = list(set(vocabulary))\n",
    "\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = len(vocab)\n",
    "print('Data size', len(vocabulary), ' Vocab Size', vocabulary_size)\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1], ('*', 12625), ('ነው', 3070), ('#', 1975), ('ላይ', 1908)]\n",
      "Sample data [2575, 2854, 1419, 10063, 1625, 10064, 3627, 352, 862, 247] ['ጋዜጠኛ', 'ተመስገን', 'ደሳለኝ', 'በጠበቃው', 'በአቶ', 'አምሐ', 'መኮንን', 'አማካይነት', 'በፌዴራል', 'ጠቅላይ']\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419 ደሳለኝ -> 2854 ተመስገን\n",
      "1419 ደሳለኝ -> 2575 ጋዜጠኛ\n",
      "10063 በጠበቃው -> 1625 በአቶ\n",
      "10063 በጠበቃው -> 10064 አምሐ\n",
      "1625 በአቶ -> 10064 አምሐ\n",
      "1625 በአቶ -> 10063 በጠበቃው\n",
      "10064 አምሐ -> 352 አማካይነት\n",
      "10064 አምሐ -> 3627 መኮንን\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "#     batch_size = batch_size // 2\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(\n",
    "        maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "            \n",
    "#     sim_batch = np.zeros_like(batch)\n",
    "#     sim_labels = np.zeros_like(labels)\n",
    "#     for b_i in range(len(batch)):\n",
    "#         b = batch[b_i]\n",
    "#         sim = sim_data[b]\n",
    "#         sim_context = np.random.choice(sim, 1)\n",
    "#         for sim_word in sim_context:\n",
    "#             sim_batch[b_i] = b\n",
    "#             sim_labels[b_i, 0] = sim_word\n",
    "    \n",
    "#     for sim_i in range(len(sim_batch)):\n",
    "#         rev_word = reverse_dictionary[sim_batch[sim_i]]\n",
    "#         rev_label = reverse_dictionary[sim_labels[sim_i, 0]]\n",
    "#     batch = np.hstack((batch, sim_batch))\n",
    "#     labels = np.vstack((labels, sim_labels))\n",
    "        \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88  4 62 63 48 85 73 84 39 47]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "np.random.seed(1000)\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "print(valid_examples[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)    \n",
    "            print(embed.get_shape())\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  283.3926086425781\n",
      "Nearest to በተለይ: ይቸገራል, ድንቅ, ኖረ, ሰብዕና, ተነገራቸው, ነጻነቷን, ስለሚከተለው, የታደገ,\n",
      "Nearest to ላይ: የተስተናገዱ, ኮንትኔንታል, መምጣታቸው, የውጭና, ከብሔራዊ, ሲባሉ, ፈላጊዎች, ከቀለጠ,\n",
      "Nearest to መካከል: የሚያማልል, ስለአገር, የተገዳደልንበት, ሲቀበል, ከቱኒዚያው, ድብልቅልቃችን, ያልቻለበትን, ውጤቱ,\n",
      "Nearest to ፓርቲዎች: ተቋቁሟል, ይፈታሉ, ለሚመለከታቸው, አቃሎና, ከተቀሰቀሰ, ሙዚቀኞች, በሸራ, ጠራርጐ,\n",
      "Nearest to አሁን: ስወርድ, ከኃይለ, በመዝራት, ማሸጊያ, የተመለሱት, ገጠመኝ, በፅናት, እያጨለመ,\n",
      "Nearest to አለበት: ሲደፋ, ስለተነጠቀች, የመክበሪያ, ካልተፈጠረ, በጣጥሶ, መሄዱን, ዜግነቱ, እስከመለየት,\n",
      "Nearest to ምክር: በጥልቀትና, ዘርግቶ, አበሻ, በየፊናው, በስልኩ, ተጨባጭነት, ለመታደም, ሶማሊያ,\n",
      "Nearest to ሳይሆን: ተቃዋሚና, ማነው, ላፕቶፕዋ, ፖለቲካል, እንዲጠበቅላትና, ከሶማሊያ, ሕይወትስ, የኪስ,\n",
      "Nearest to መሆኑን: ያበጃል, ችሎቶች, ብዙነሽ, እንዲታገላቸው, የሚደውሉ, ይኖሩ, ዜናዊን, ሰውነታቸው,\n",
      "Nearest to ችግር: ቢተውን, በየእለቱ, ሳናይማ, ታሳቢነት, ማግበስበሻ, መሐሙድና, መሰባበር, ድብደባና,\n",
      "Nearest to ከ#: አርሚን, ችግርከማዕከላዊ, የመጨረሻውን, መቀለድ, ሊያደርግበት, ምዝገባው, እንደነገሩን, መዋላቸውን,\n",
      "Nearest to ሚሊዮን: የንድፊያው, በተከሰተባቸው, ወደሆነውና, ብዙዎችም, እየነዱ, ያቀድነው, ሥራቸው, በተከበበ,\n",
      "Nearest to ደረጃ: አይቀበልም, ይገዛሉ, በመሠራቱ, ኮሪያውያንን, ግንኙነቱም, ያመጣበትን, ፐርሴፕሽን, የሚደረጉት,\n",
      "Nearest to ከፍተኛ: መነሳሳትን, እንዳይፈልቁ, ስንቃኘው, ዕውቀት, እንጅ, ለማገልገል, ፎረስት, የሚያደርስ,\n",
      "Nearest to ቀን: ጉበኛ, ሲያብራራ, አምላኩን, እናላችሁ, መመዝገቡ, መርገጥ, ሊያጐናጽፍ, በዋናነት,\n",
      "Nearest to ጋር: ቃኝተው, ተማሪ, ድሆችን, መቀረፍ, በቁማቸው, የቀረቡላቸውን, መድረክን, እንዳይወድቁ,\n",
      "Average loss at step  2000 :  125.10967356872558\n",
      "Average loss at step  4000 :  59.758877345561984\n",
      "Average loss at step  6000 :  33.71604934120178\n",
      "Average loss at step  8000 :  22.030453946113585\n",
      "Average loss at step  10000 :  14.392084807634353\n",
      "Nearest to በተለይ: ይቸገራል, የተያያዘ, ፔትሮሊየም, ድንቅ, ወጣቶቻችንን, በስሜት, እንዲውል, ይጥላሉ,\n",
      "Nearest to ላይ: ከብሔራዊ, ሲባሉ, ሲተመንም, የተስተናገዱ, ድካም, በሶማሌ, የቀለጠ, ለመቆፈርና,\n",
      "Nearest to መካከል: ውጤቱ, የሚያማልል, ማኅበረሰባዊ, አድማሱ, ብርሃነየሱስን, ከቱኒዚያው, ስለአገር, ያልቻለበትን,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, ተቋቁሟል, ይፈታሉ, ያጋባል, ከሆኑ, ብልጫ, መኪናችን, በትክክል,\n",
      "Nearest to አሁን: የሚገባው, የጀመርነው, በፅናት, አገልግሎትን, ላላቸው, የአፍሪካ, ገጠመኝ, ክፍያ,\n",
      "Nearest to አለበት: ሲደፋ, ስለተነጠቀች, ካልተፈጠረ, የሚቃረኑ, በብስክሌት, ለዓለም, ዜግነቱ, የሚተካ,\n",
      "Nearest to ምክር: በጥልቀትና, ዘርግቶ, በተሻሻለው, በየፊናው, ሽመልስ, ባለመታወቁ, ተሰብስባችሁ, የእኛ,\n",
      "Nearest to ሳይሆን: የሚታይባቸው, ፖለቲካል, ማነው, የኪስ, ኤፍሬም, ተቃዋሚና, የማስታወቂያ, ሕይወትስ,\n",
      "Nearest to መሆኑን: አሉኝ, በአማራ, ሳቢያ, ያበጃል, ጠርዝ, የሚደውሉ, ከሥሩ, ዜናዊን,\n",
      "Nearest to ችግር: ትግሉ, መጠየቅ, ኢትዮጵያዊነትን, ድምፃቸውን, ካርቱም, የባንኩን, ከፓርቲ, ቢተውን,\n",
      "Nearest to ከ#: #, አርሚን, ችግርከማዕከላዊ, ለማሟላት, ካስነሳችው, ይካሄዳል, ምዝገባው, አቀረቡ,\n",
      "Nearest to ሚሊዮን: የሚታወቁት, ውድቅ, ሥራቸው, ዘላቂ, ቢቻልም, ማክሰኞ, ይጠቁማል, ብዙዎችም,\n",
      "Nearest to ደረጃ: አይቀበልም, የሚደረጉት, ታዋቂው, ለመሥራት, በጁቡቲ, በስንት, የተቀመጠው, የመሥሪያ,\n",
      "Nearest to ከፍተኛ: እንጅ, ዕውቀት, ፎረስት, ስንቃኘው, ለማገልገል, መደበኛ, ትራንስፎርሜሽን, ቢጂአይ,\n",
      "Nearest to ቀን: ጉበኛ, በዋናነት, ሲያብራራ, አንቀጽ, ሐሳብን, እስከሆነ, በትብብር, ያስተምር,\n",
      "Nearest to ጋር: ቃኝተው, ተማሪ, በምልዓተ, ድሆችን, ሳናድበሰብስ, ጉባዔው, ወግ, ከኢትዮከለር,\n",
      "Average loss at step  12000 :  10.127957687854767\n",
      "Average loss at step  14000 :  8.166958601236344\n",
      "Average loss at step  16000 :  6.77840860593319\n",
      "Average loss at step  18000 :  5.851044003605843\n",
      "Average loss at step  20000 :  5.260726887464523\n",
      "Nearest to በተለይ: ይቸገራል, ወጣቶቻችንን, የተያያዘ, ፔትሮሊየም, ነጻነቷን, ሰብዕና, ውሎ, ካባ,\n",
      "Nearest to ላይ: የተስተናገዱ, ከብሔራዊ, ለመቆፈርና, ሲባሉ, ሲተመንም, የቀለጠ, ድካም, እየሳበች,\n",
      "Nearest to መካከል: ውጤቱ, የሚያማልል, ብርሃነየሱስን, ማኅበረሰባዊ, ከቱኒዚያው, ለመሰየም, ተከታያቸው, አድማሱ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, ያጋባል, ይፈታሉ, ከጭንቅላቴ, አቃሎና, ተቋቁሟል, ብልጫ, በስማቸው,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, የሚገባው, ስወርድ, ከኃይለ, ከአንጋፋ, የምቀርበው, እያጨለመ, ለቀረበው,\n",
      "Nearest to አለበት: ሲደፋ, ካልተፈጠረ, ስለተነጠቀች, በብስክሌት, የመክበሪያ, የሚቃረኑ, ለምክትላቸው, ልታቆመው,\n",
      "Nearest to ምክር: በጥልቀትና, ዘርግቶ, ተሰብስባችሁ, ባለመታወቁ, በተሻሻለው, በየፊናው, ነጋዴዎችም, መቻቻልና,\n",
      "Nearest to ሳይሆን: የኪስ, የሚታይባቸው, ትግበራና, እንዲጠበቅላትና, ተቃዋሚና, ልማዳዊ, የማስታወቂያ, ከሲቪል,\n",
      "Nearest to መሆኑን: አሉኝ, የሚደውሉ, ያበጃል, ጠርዝ, በአማራ, ሳቢያ, ዜናዊን, ከሥሩ,\n",
      "Nearest to ችግር: ትግሉ, ቢተውን, ድምፃቸውን, መጠየቅ, ኢትዮጵያዊነትን, ማግበስበሻ, ጨፍልቆ, ካርቱም,\n",
      "Nearest to ከ#: #, አርሚን, ችግርከማዕከላዊ, ካስነሳችው, መዋላቸውን, አቀረቡ, የሕዝባችን, የአካላቸው,\n",
      "Nearest to ሚሊዮን: የሚታወቁት, #, ውድቅ, ብዙዎችም, ለማምሸት, በተከሰተባቸው, ሥራቸው, ቢቻልም,\n",
      "Nearest to ደረጃ: አይቀበልም, በጁቡቲ, የሚደረጉት, ፐርሴፕሽን, ታዋቂው, ያላስደረጉ, የመሥሪያ, በሌለሁበትና,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, ፎረስት, ለማገልገል, ዕውቀት, ተጠባቢነቱ, መነሳሳትን, እየተመቻቸ,\n",
      "Nearest to ቀን: ሲያብራራ, ጉበኛ, በዋናነት, ታሳቢ, ኅዳር, ያስተምር, እናላችሁ, ሳይፈጅ,\n",
      "Nearest to ጋር: ቃኝተው, ተማሪ, ድሆችን, ከመሸማቀቅ, በምልዓተ, የቀረቡላቸውን, ሳናድበሰብስ, መቀረፍ,\n",
      "Average loss at step  22000 :  4.794485333323479\n",
      "Average loss at step  24000 :  4.461835022330284\n",
      "Average loss at step  26000 :  4.238062541842461\n",
      "Average loss at step  28000 :  4.072592362880707\n",
      "Average loss at step  30000 :  3.9418229044675828\n",
      "Nearest to በተለይ: ይቸገራል, ወጣቶቻችንን, ፔትሮሊየም, ከእንግሊዙ, ነጻነቷን, ሰብዕና, ተነገራቸው, ካባ,\n",
      "Nearest to ላይ: የተስተናገዱ, ለመቆፈርና, ሲተመንም, ሲባሉ, የቀለጠ, እየሳበች, ፈላጊዎች, ከብሔራዊ,\n",
      "Nearest to መካከል: የሚያማልል, ውጤቱ, ብርሃነየሱስን, ማኅበረሰባዊ, ተከታያቸው, ለመሰየም, ሲጥሱ, ከቱኒዚያው,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, አቃሎና, ያጋባል, ይፈታሉ, ከጭንቅላቴ, መታማትን, ተቋቁሟል, በስማቸው,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ከኃይለ, የሚገባው, ስወርድ, ከአንጋፋ, ለቀረበው, የምቀርበው, አገልግሎትን,\n",
      "Nearest to አለበት: ሲደፋ, ስለተነጠቀች, ካልተፈጠረ, በብስክሌት, ለምክትላቸው, በጣጥሶ, ይሆናል, ልታቆመው,\n",
      "Nearest to ምክር: ጽሕፈት, በጥልቀትና, ተሰብስባችሁ, ዘርግቶ, ባለመታወቁ, ይብሰል, በየፊናው, ነጋዴዎችም,\n",
      "Nearest to ሳይሆን: የኪስ, ትግበራና, የሚታይባቸው, ልማዳዊ, እንዲጠበቅላትና, ተቃዋሚና, ከሲቪል, አዝማድና,\n",
      "Nearest to መሆኑን: አሉኝ, የሚደውሉ, እንደሆነ, ያበጃል, ጠርዝ, ዕድገት, ሳቢያ, ዜናዊን,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ቢተውን, መጠየቅ, ማግበስበሻ, ኢትዮጵያዊነትን, ጨፍልቆ, ካርቱም,\n",
      "Nearest to ከ#: ካስነሳችው, አርሚን, #, ችግርከማዕከላዊ, መዋላቸውን, አቀረቡ, ክርስቲያኖች, የሕዝባችን,\n",
      "Nearest to ሚሊዮን: የሚታወቁት, #, ለማምሸት, መንበር, ስማቸውን, ቢሊዮን, መክፈል, ኮ,\n",
      "Nearest to ደረጃ: በጁቡቲ, አይቀበልም, ያላስደረጉ, በሌለሁበትና, የሚደረጉት, ፐርሴፕሽን, ታዋቂው, የሚላኩና,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, ፎረስት, እየተመቻቸ, ተጠባቢነቱ, ለማገልገል, ዕውቀት, ሲያሸንፍ,\n",
      "Nearest to ቀን: ሲያብራራ, ጉበኛ, ኅዳር, ታሳቢ, ያስተምር, በዋናነት, አደናቀፈኝ, መማማር,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, በምልዓተ, የቀረቡላቸውን, ሳናድበሰብስ, መቀረፍ,\n",
      "Average loss at step  32000 :  3.7924044841527937\n",
      "Average loss at step  34000 :  3.6603193261623383\n",
      "Average loss at step  36000 :  3.6022588268518447\n",
      "Average loss at step  38000 :  3.5002064672708513\n",
      "Average loss at step  40000 :  3.4408683379888534\n",
      "Nearest to በተለይ: ይቸገራል, ወጣቶቻችንን, ከእንግሊዙ, ፔትሮሊየም, ነጻነቷን, ሳይተጓጎል, ተነገራቸው, ካባ,\n",
      "Nearest to ላይ: የተስተናገዱ, ለመቆፈርና, ሲባሉ, ሲተመንም, እየሳበች, የቀለጠ, ፈላጊዎች, ታይዋን,\n",
      "Nearest to መካከል: የሚያማልል, ብርሃነየሱስን, ማኅበረሰባዊ, ሲጥሱ, ተከታያቸው, ለመሰየም, ውጤቱ, ማናለብኝነት,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, አቃሎና, ያጋባል, ከጭንቅላቴ, መታማትን, ይፈታሉ, የእስር, ኤቲኤምና,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ከኃይለ, ለቀረበው, ከአንጋፋ, የሚገባው, ስወርድ, የምቀርበው, አውሮፓና,\n",
      "Nearest to አለበት: ሲደፋ, ስለተነጠቀች, ካልተፈጠረ, በብስክሌት, ለምክትላቸው, ይሆናል, በጣጥሶ, ልታቆመው,\n",
      "Nearest to ምክር: ጽሕፈት, ተሰብስባችሁ, በጥልቀትና, ዘርግቶ, በእጁ, ይብሰል, እንዳደረጉትም, ተጨባጭነት,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የኪስ, ትግበራና, የሚታይባቸው, እንዲጠበቅላትና, ተቃዋሚና, የሰማዩ, አዝማድና,\n",
      "Nearest to መሆኑን: አሉኝ, እንደሆነ, የሚደውሉ, ያበጃል, ዕድገት, ጠርዝ, ስሟ, የማይቆም,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ማግበስበሻ, ቢተውን, መጠየቅ, ኢትዮጵያዊነትን, እየተገለጸ, በየእለቱ,\n",
      "Nearest to ከ#: ካስነሳችው, አርሚን, መዋላቸውን, ችግርከማዕከላዊ, #, አቀረቡ, የሕዝባችን, ክርስቲያኖች,\n",
      "Nearest to ሚሊዮን: የሚታወቁት, ለማምሸት, ቢሊዮን, #, ሺሕ, መንበር, አሜባ, ስምንተኛና,\n",
      "Nearest to ደረጃ: በጁቡቲ, አይቀበልም, ያላስደረጉ, የሚደረጉት, በሌለሁበትና, የሚላኩና, ታዋቂው, በመግቢያዬ,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, እየተመቻቸ, እየዳረገ, ፎረስት, ተጠባቢነቱ, ሲያሸንፍ, ይደፍራሉ,\n",
      "Nearest to ቀን: ሲያብራራ, ጉበኛ, ኅዳር, አደናቀፈኝ, መማማር, ታሳቢ, ያስተምር, እናላችሁ,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, በምልዓተ, መቀረፍ, የቀረቡላቸውን, በከንቱ,\n",
      "Average loss at step  42000 :  3.388431841492653\n",
      "Average loss at step  44000 :  3.3198532128334044\n",
      "Average loss at step  46000 :  3.25686161506176\n",
      "Average loss at step  48000 :  3.2307372612953187\n",
      "Average loss at step  50000 :  3.165687270998955\n",
      "Nearest to በተለይ: ይቸገራል, ከእንግሊዙ, ወጣቶቻችንን, ፔትሮሊየም, ነጻነቷን, ሳይተጓጎል, ተነገራቸው, ተገጣጣሚ,\n",
      "Nearest to ላይ: የተስተናገዱ, ለመቆፈርና, ሲተመንም, ሲባሉ, ፈላጊዎች, የቀለጠ, ሀገሮች, ታይዋን,\n",
      "Nearest to መካከል: የሚያማልል, ማኅበረሰባዊ, ብርሃነየሱስን, ሲጥሱ, ለመሰየም, ተከታያቸው, ሚኒስትሮችም, ውጤቱ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, ያጋባል, መታማትን, አቃሎና, ከጭንቅላቴ, የእስር, ይፈታሉ, ኤቲኤምና,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ለቀረበው, ከኃይለ, ከአንጋፋ, አውሮፓና, የምቀርበው, እንዲህ, የሚገባው,\n",
      "Nearest to አለበት: ሲደፋ, በብስክሌት, ስለተነጠቀች, ካልተፈጠረ, ለምክትላቸው, ይሆናል, በጣጥሶ, ልታቆመው,\n",
      "Nearest to ምክር: ጽሕፈት, ተሰብስባችሁ, በእጁ, በጥልቀትና, ይብሰል, እንዳደረጉትም, ሰበር, ዘርግቶ,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የኪስ, ትግበራና, የሚታይባቸው, እንዲጠበቅላትና, የሰማዩ, ተቃዋሚና, አዝማድና,\n",
      "Nearest to መሆኑን: እንደሆነ, አሉኝ, የሚደውሉ, ዕድገት, ያበጃል, ጠርዝ, ስሟ, የማይቆም,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ማግበስበሻ, መጠየቅ, ኢትዮጵያዊነትን, ቢተውን, እየተገለጸ, በየእለቱ,\n",
      "Nearest to ከ#: ካስነሳችው, አርሚን, መዋላቸውን, ችግርከማዕከላዊ, የሕዝባችን, አቀረቡ, #, ከመጋቢት,\n",
      "Nearest to ሚሊዮን: ቢሊዮን, ሺሕ, የሚታወቁት, ለማምሸት, አሜባ, ስምንተኛና, መክፈል, ኪሎ,\n",
      "Nearest to ደረጃ: በጁቡቲ, አይቀበልም, ያላስደረጉ, በሌለሁበትና, በመግቢያዬ, የሚደረጉት, የሚላኩና, ታዋቂው,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, እየተመቻቸ, እየዳረገ, ተጠባቢነቱ, ሲያሸንፍ, ይደፍራሉ, ፎረስት,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, ኅዳር, ጉበኛ, መማማር, እናላችሁ, ሊያጐናጽፍ, ነሐሴ,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, መቀረፍ, ከመሸማቀቅ, የቀረቡላቸውን, አያይዘው, በምልዓተ,\n",
      "Average loss at step  52000 :  3.1149801565408706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  54000 :  3.094681325793266\n",
      "Average loss at step  56000 :  3.0309101486206056\n",
      "Average loss at step  58000 :  3.0227978419065478\n",
      "Average loss at step  60000 :  2.9798253293037416\n",
      "Nearest to በተለይ: ይቸገራል, ከእንግሊዙ, ወጣቶቻችንን, ሳይተጓጎል, ፔትሮሊየም, ነጻነቷን, ተነገራቸው, ተገጣጣሚ,\n",
      "Nearest to ላይ: የተስተናገዱ, ለመቆፈርና, ሲተመንም, ሲባሉ, የቀለጠ, ፈላጊዎች, እየሳበች, ሀገሮች,\n",
      "Nearest to መካከል: የሚያማልል, ማኅበረሰባዊ, ሲጥሱ, ብርሃነየሱስን, ለመሰየም, ሚኒስትሮችም, ተከታያቸው, የተቋጨ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, መታማትን, አቃሎና, ያጋባል, ኤቲኤምና, የእስር, ከጭንቅላቴ, ይፈታሉ,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ለቀረበው, ከአንጋፋ, አውሮፓና, ከኃይለ, እንዲህ, እየቆጠቆጠ, በመሆኑም,\n",
      "Nearest to አለበት: ሲደፋ, በብስክሌት, ስለተነጠቀች, ይሆናል, ለምክትላቸው, ካልተፈጠረ, አለባቸው, የለበትም,\n",
      "Nearest to ምክር: ጽሕፈት, ተሰብስባችሁ, በእጁ, ሰበር, በጥልቀትና, እንዳደረጉትም, ተጨባጭነት, ይብሰል,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የኪስ, የሚታይባቸው, ትግበራና, የሰማዩ, እንዲጠበቅላትና, ተቃዋሚና, አዝማድና,\n",
      "Nearest to መሆኑን: እንደሆነ, አሉኝ, የሚደውሉ, ዕድገት, የማይቆም, ያበጃል, ጠርዝ, ስሟ,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ማግበስበሻ, እየተገለጸ, መጠየቅ, በጉልህ, ዩክሬናዊው, ኢትዮጵያዊነትን,\n",
      "Nearest to ከ#: ካስነሳችው, አርሚን, መዋላቸውን, ከመጋቢት, የሕዝባችን, ችግርከማዕከላዊ, #, አቀረቡ,\n",
      "Nearest to ሚሊዮን: ሺሕ, ቢሊዮን, የሚታወቁት, ለማምሸት, ኪሎ, አሜባ, ስምንተኛና, በሶቭየት,\n",
      "Nearest to ደረጃ: በጁቡቲ, በመግቢያዬ, ያላስደረጉ, አይቀበልም, የሚደረጉት, የሚላኩና, በሌለሁበትና, አስነስቷል,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, እየዳረገ, እየተመቻቸ, ተጠባቢነቱ, ይደፍራሉ, በቀይ, በወዳደቁ,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, ጉበኛ, ኅዳር, መማማር, እናላችሁ, ሊያጐናጽፍ, እንደነበርና,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, ሳናድበሰብስ, የቀረቡላቸውን, መቀረፍ, አያይዘው,\n",
      "Average loss at step  62000 :  2.920186127662659\n",
      "Average loss at step  64000 :  2.9216976066827773\n",
      "Average loss at step  66000 :  2.886561287522316\n",
      "Average loss at step  68000 :  2.833697946310043\n",
      "Average loss at step  70000 :  2.837986863553524\n",
      "Nearest to በተለይ: ይቸገራል, ከእንግሊዙ, ሳይተጓጎል, ወጣቶቻችንን, ተገጣጣሚ, ፔትሮሊየም, ተነገራቸው, ነጻነቷን,\n",
      "Nearest to ላይ: የተስተናገዱ, ሲተመንም, ለመቆፈርና, ፈላጊዎች, የቀለጠ, ሲባሉ, እየሳበች, ሀገሮች,\n",
      "Nearest to መካከል: የሚያማልል, ማኅበረሰባዊ, ሲጥሱ, ብርሃነየሱስን, ሚኒስትሮችም, ለመሰየም, ለመመገብ, የተቋጨ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, መታማትን, ያጋባል, አቃሎና, የእስር, ይፈታሉ, ከጭንቅላቴ, ኤቲኤምና,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, አውሮፓና, ከአንጋፋ, ለቀረበው, ከኃይለ, በመሆኑም, እንዲህ, እየቆጠቆጠ,\n",
      "Nearest to አለበት: ሲደፋ, በብስክሌት, ስለተነጠቀች, ይሆናል, ለምክትላቸው, የለበትም, አለባቸው, ካልተፈጠረ,\n",
      "Nearest to ምክር: ጽሕፈት, በእጁ, ተሰብስባችሁ, እንደሚያቀርቡ, ሰበር, እንዳደረጉትም, ይብሰል, ተጨባጭነት,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የሚታይባቸው, ትግበራና, የኪስ, የሰማዩ, ተቃዋሚና, እንዲጠበቅላትና, አዝማድና,\n",
      "Nearest to መሆኑን: እንደሆነ, የሚደውሉ, አሉኝ, ዕድገት, የማይቆም, ስሟ, ጠርዝ, ያበጃል,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ማግበስበሻ, እየተገለጸ, ዩክሬናዊው, እንዲቀየር, መጠየቅ, በጉልህ,\n",
      "Nearest to ከ#: ካስነሳችው, አርሚን, መዋላቸውን, ከመጋቢት, የሕዝባችን, ችግርከማዕከላዊ, ያበረከቱ, አቀረቡ,\n",
      "Nearest to ሚሊዮን: ሺሕ, ቢሊዮን, የሚታወቁት, ኪሎ, አሜባ, ለማምሸት, በሶቭየት, ስምንተኛና,\n",
      "Nearest to ደረጃ: በጁቡቲ, በመግቢያዬ, ያላስደረጉ, የሚላኩና, የሚደረጉት, አይቀበልም, በሌለሁበትና, አስነስቷል,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, እየዳረገ, በቀይ, በወዳደቁ, እየተመቻቸ, ተጠባቢነቱ, ይደፍራሉ,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, ጉበኛ, እንደነበርና, ኅዳር, መማማር, እናላችሁ, እያጎላው,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, ሳናድበሰብስ, መቀረፍ, የቀረቡላቸውን, አሊ,\n",
      "Average loss at step  72000 :  2.7931118232011793\n",
      "Average loss at step  74000 :  2.77188634455204\n",
      "Average loss at step  76000 :  2.7575370814800264\n",
      "Average loss at step  78000 :  2.705370146691799\n",
      "Average loss at step  80000 :  2.721701702415943\n",
      "Nearest to በተለይ: ከእንግሊዙ, ይቸገራል, ሳይተጓጎል, ተገጣጣሚ, ወጣቶቻችንን, ፔትሮሊየም, ተነገራቸው, ነጻነቷን,\n",
      "Nearest to ላይ: የተስተናገዱ, ሲተመንም, ለመቆፈርና, ፈላጊዎች, የቀለጠ, ሲባሉ, ሀገሮች, እየሳበች,\n",
      "Nearest to መካከል: ማኅበረሰባዊ, የሚያማልል, ሲጥሱ, ሚኒስትሮችም, ብርሃነየሱስን, ለመሰየም, ለመመገብ, የተቋጨ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, መታማትን, ያጋባል, ተረጋግቷል, አቃሎና, የእስር, በምታምነው, ተንታኞች,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ለቀረበው, አውሮፓና, ከአንጋፋ, በመሆኑም, ከኃይለ, እንዲህ, የተለዋወጡ,\n",
      "Nearest to አለበት: በብስክሌት, ሲደፋ, አለባቸው, የለበትም, ስለተነጠቀች, ይሆናል, ለምክትላቸው, አለብን,\n",
      "Nearest to ምክር: ጽሕፈት, በእጁ, ተሰብስባችሁ, እንደሚያቀርቡ, ሰበር, እንዳደረጉትም, ተጨባጭነት, የሚጠቀምባቸው,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የሚታይባቸው, የኪስ, ትግበራና, የሰማዩ, ተቃዋሚና, እንዲጠበቅላትና, አዝማድና,\n",
      "Nearest to መሆኑን: እንደሆነ, የሚደውሉ, አሉኝ, የማይቆም, ዕድገት, ስሟ, ጠርዝ, ያበጃል,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ማግበስበሻ, ዩክሬናዊው, እየተገለጸ, እንዲቀየር, በጉልህ, ኢትዮጵያዊነትን,\n",
      "Nearest to ከ#: ካስነሳችው, መዋላቸውን, አርሚን, የሕዝባችን, ከመጋቢት, አቀረቡ, አንስተንለታል, ባዘዘው,\n",
      "Nearest to ሚሊዮን: ሺሕ, ቢሊዮን, የሚታወቁት, ኪሎ, በሶቭየት, ስምንተኛና, አሜባ, ለማምሸት,\n",
      "Nearest to ደረጃ: በመግቢያዬ, በጁቡቲ, ያላስደረጉ, የሚላኩና, የሚደረጉት, በሌለሁበትና, አይቀበልም, አስነስቷል,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እንጅ, እየዳረገ, በወዳደቁ, ይደፍራሉ, እየተመቻቸ, በቀይ, አይናቸው,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, እንደነበርና, እናላችሁ, መማማር, ጉበኛ, ኅዳር, እያጎላው,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, ሳናድበሰብስ, መቀረፍ, የቀረቡላቸውን, አሊ,\n",
      "Average loss at step  82000 :  2.685499468743801\n",
      "Average loss at step  84000 :  2.6358895946741105\n",
      "Average loss at step  86000 :  2.6478020541667937\n",
      "Average loss at step  88000 :  2.6236234169006347\n",
      "Average loss at step  90000 :  2.5787875114679335\n",
      "Nearest to በተለይ: ከእንግሊዙ, ይቸገራል, ሳይተጓጎል, ተገጣጣሚ, ወጣቶቻችንን, ተነገራቸው, ፔትሮሊየም, ነጻነቷን,\n",
      "Nearest to ላይ: የተስተናገዱ, ሲተመንም, ለመቆፈርና, ፈላጊዎች, የቀለጠ, ሲባሉ, እየሳበች, ሀገሮች,\n",
      "Nearest to መካከል: ማኅበረሰባዊ, ሲጥሱ, የሚያማልል, ሚኒስትሮችም, ብርሃነየሱስን, ለመሰየም, የተቋጨ, ለመመገብ,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, መታማትን, ተረጋግቷል, ያጋባል, በምታምነው, ተንታኞች, አቃሎና, ልንተባበር,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, ከአንጋፋ, አውሮፓና, በመሆኑም, የተለዋወጡ, እየቆጠቆጠ, እንዲህ, ለቀረበው,\n",
      "Nearest to አለበት: አለባቸው, በብስክሌት, የለበትም, ሲደፋ, ይሆናል, አለብን, ስለተነጠቀች, ለምክትላቸው,\n",
      "Nearest to ምክር: ጽሕፈት, በእጁ, እንደሚያቀርቡ, ተሰብስባችሁ, ሰበር, እንዳደረጉትም, ሶማሊያ, መሥራችና,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የሚታይባቸው, ትግበራና, የኪስ, የሰማዩ, ተቃዋሚና, እንዲጠበቅላትና, ክብርና,\n",
      "Nearest to መሆኑን: እንደሆነ, የሚደውሉ, አሉኝ, የማይቆም, ዕድገት, ስሟ, ጠርዝ, ከሚስቡ,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ዩክሬናዊው, እየተገለጸ, ማግበስበሻ, እንዲቀየር, በጉልህ, በየእለቱ,\n",
      "Nearest to ከ#: ካስነሳችው, መዋላቸውን, አርሚን, ከመጋቢት, የሕዝባችን, አንስተንለታል, ባዘዘው, ከዚህስ,\n",
      "Nearest to ሚሊዮን: ሺሕ, ቢሊዮን, ስምንተኛና, የሚታወቁት, ኪሎ, በሶቭየት, አሜባ, ለማምሸት,\n",
      "Nearest to ደረጃ: በመግቢያዬ, በጁቡቲ, የሚላኩና, ያላስደረጉ, የሚደረጉት, አስነስቷል, በሌለሁበትና, አይቀበልም,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እየዳረገ, በወዳደቁ, እንጅ, በቀይ, ይደፍራሉ, እየተመቻቸ, አይናቸው,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, እንደነበርና, እናላችሁ, ኅዳር, መማማር, ጉበኛ, እያጎላው,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ከመሸማቀቅ, የቀረቡላቸውን, ሳናድበሰብስ, ስህተቱና, መቀረፍ,\n",
      "Average loss at step  92000 :  2.591977008163929\n",
      "Average loss at step  94000 :  2.5376808247566225\n",
      "Average loss at step  96000 :  2.5558287448883057\n",
      "Average loss at step  98000 :  2.532930596649647\n",
      "Average loss at step  100000 :  2.471836886167526\n",
      "Nearest to በተለይ: ከእንግሊዙ, ሳይተጓጎል, ተገጣጣሚ, ይቸገራል, ተነገራቸው, ፔትሮሊየም, ወጣቶቻችንን, በምሥራቅ,\n",
      "Nearest to ላይ: ሲተመንም, የተስተናገዱ, ለመቆፈርና, የቀለጠ, ፈላጊዎች, እየሳበች, ሲባሉ, ሀገሮች,\n",
      "Nearest to መካከል: ማኅበረሰባዊ, ሲጥሱ, የሚያማልል, ሚኒስትሮችም, የተቋጨ, ብርሃነየሱስን, ለመመገብ, ለመሰየም,\n",
      "Nearest to ፓርቲዎች: ሙዚቀኞች, መታማትን, ተረጋግቷል, ያጋባል, በምታምነው, ተንታኞች, ከሆኑ, ኦፌኮ,\n",
      "Nearest to አሁን: ማዥጎድጎዳቸው, በመሆኑም, ከአንጋፋ, አውሮፓና, የተለዋወጡ, እንዲህ, ማካካሻ, እየቆጠቆጠ,\n",
      "Nearest to አለበት: የለበትም, አለባቸው, በብስክሌት, ሲደፋ, አለብን, ይሆናል, ለምክትላቸው, ስለተነጠቀች,\n",
      "Nearest to ምክር: ጽሕፈት, በእጁ, እንደሚያቀርቡ, ሰበር, እንዳደረጉትም, ተሰብስባችሁ, መሥራችና, ተጨባጭነት,\n",
      "Nearest to ሳይሆን: ልማዳዊ, የሚታይባቸው, የሰማዩ, ትግበራና, የኪስ, ተቃዋሚና, እንዲጠበቅላትና, ወገኖችን,\n",
      "Nearest to መሆኑን: እንደሆነ, የሚደውሉ, አሉኝ, ዕድገት, የማይቆም, ከሚስቡ, ከወርኃዊ, የገለጸውና,\n",
      "Nearest to ችግር: ትግሉ, ድምፃቸውን, ዩክሬናዊው, እየተገለጸ, ማግበስበሻ, እንዲቀየር, በየእለቱ, በጉልህ,\n",
      "Nearest to ከ#: ካስነሳችው, ከመጋቢት, አርሚን, መዋላቸውን, የሕዝባችን, ባዘዘው, አንስተንለታል, ከዚህስ,\n",
      "Nearest to ሚሊዮን: ሺሕ, ቢሊዮን, ኪሎ, ስምንተኛና, በሶቭየት, የሚታወቁት, ለማምሸት, አሜባ,\n",
      "Nearest to ደረጃ: በመግቢያዬ, በጁቡቲ, የሚላኩና, የሚደረጉት, ያላስደረጉ, አስነስቷል, በሌለሁበትና, ሁከቱ,\n",
      "Nearest to ከፍተኛ: ስንቃኘው, እየዳረገ, በወዳደቁ, በቀይ, እንጅ, ሌብነቱ, ይደፍራሉ, አይናቸው,\n",
      "Nearest to ቀን: ሲያብራራ, አደናቀፈኝ, እንደነበርና, የሚያርፍበት, እናላችሁ, ኅዳር, መማማር, ጉበኛ,\n",
      "Nearest to ጋር: ቃኝተው, ድሆችን, ተማሪ, ሳናድበሰብስ, ከመሸማቀቅ, ስህተቱና, አሊ, መቀረፍ,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter('./log', session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                    skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open('./log/metadata.tsv', 'w', encoding='utf8') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join('./log', 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join('./log', 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.rc('font', family='Abyssinica SIL')\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(\n",
    "        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, 'tsne.png')\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
